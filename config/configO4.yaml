# M = multi
wandb_project: semisupcon_bestloss
save_name: O4
algorithm: semisupconproto
dataset: cifar100
loss: SemiSupConLoss # Supcon&Simclr #OnlySupcon                        #all_withoutsimclr #all_withoutsimclr_withoutunsupce # all #only_unsup (for pretraining, and resume = True)
lambda_proto: 1 #only for Weighted supcon loss
lambda_yup: 1.2 #only for Weighted supcon loss
lambda_ydown: .2 #only for Weighted supcon loss
pl: softmax # softmax
pl_temp: .04
use_cat: True # if false allows to detach
save_dir: ./saved_models/classic_cv
resume: True
droprate: 0
resume_only_weight: False # When resuming a model, only take the weight for finetuning (make epochs,scheduler, optimizer start from scratch)
load_path: auto #latest_model.pth epoch124
overwrite: True
use_tensorboard: True
use_wandb: True
epoch: 256 #256 #1024 #512
num_train_iter: 262144 #262144 # 1048576 #524288
num_eval_iter: 5120 #5120
num_log_iter: 5120 #256
num_labels: 400 # 100 2500cifar100
batch_size: 64 # 64
eval_batch_size: 256
hard_label: True
T: 0.5
p_cutoff: 0.95 # 0.7 ou .95 pour fixmatchtau for pseudo labeling
ulb_loss_ratio: 1.0
uratio: 7
ema_m: 0.999
crop_ratio: 0.875
img_size: 32
optim: SGD
lr: 0.03
momentum: 0.9
weight_decay: 0.001
layer_decay: 1.0
amp: True
clip: 0.0
net: wrn_28_2_proto
net_from_name: False # If True, net_builder takes models in torch.vision models (False de base)
data_dir: ./data
train_sampler: RandomSampler
num_workers: 1
seed: 0
world_size: 1
rank: 0
multiprocessing_distributed: False
dist_url: tcp://127.0.0.1:10008
dist_backend: nccl
gpu: None




