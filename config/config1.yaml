algorithm: semisupcon
loss: all_withoutsimclr_unsupcedetached # all #only_unsup (for pretraining, and resume = True)
save_dir: ./saved_models/classic_cv
save_name: semisupcon1.2 (all_withoutsimclr_unsupcedetached) #(pretrained:simclr124epochs) #semisupcon 1.2 (without simclr) # (with 0.5*Simclr) # (0.5*Simclr) # semisupcon1.2 (withoutsimclr) (pretrained:simclr124epochs) #(without simclr)
resume: False
resume_only_weight: True # When resuming a model, only take the weight for finetuning (make epochs,scheduler, optimizer start from scratch)
load_path: ./saved_models/pretrained/pretrainedSimCLRcifar100wrn_28_2_epochs256/epoch124.pth #latest_model.pth epoch124
overwrite: True
use_tensorboard: True
use_wandb: True
wandb_project: semisupcon_cifar100wrn_28_2
epoch: 256
num_train_iter: 262144
num_eval_iter: 5120
num_log_iter: 256
num_labels: 2500 # 100 2500f
batch_size: 64 # TODO
eval_batch_size: 256
hard_label: True
T: 0.5
p_cutoff: 0.95
ulb_loss_ratio: 1.0
uratio: 7
ema_m: 0.999
crop_ratio: 0.875
img_size: 32
optim: SGD
lr: 0.03
momentum: 0.9
weight_decay: 0.0005
layer_decay: 1.0
amp: False
clip: 0.0
use_cat: True
net: wrn_28_2
net_from_name: False # If True, net_buidler takes models in torch.vision models.
data_dir: ./data
dataset: cifar100
train_sampler: RandomSampler
num_classes: 100
num_workers: 1
seed: 0
world_size: 1
rank: 0
multiprocessing_distributed: True
dist_url: tcp://127.0.0.1:10008
dist_backend: nccl
gpu: None

